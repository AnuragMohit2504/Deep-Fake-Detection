{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cfd0926-beaa-45ac-af9d-0119b8b202a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\conda_envs\\btd\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: torchvision in d:\\conda_envs\\btd\\lib\\site-packages (0.21.0+cu118)\n",
      "Requirement already satisfied: torchaudio in d:\\conda_envs\\btd\\lib\\site-packages (2.6.0+cu118)\n",
      "Requirement already satisfied: einops in d:\\conda_envs\\btd\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: pillow in d:\\conda_envs\\btd\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: tqdm in d:\\conda_envs\\btd\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: accelerate in d:\\conda_envs\\btd\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy in d:\\conda_envs\\btd\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: matplotlib in d:\\conda_envs\\btd\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: filelock in d:\\conda_envs\\btd\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\conda_envs\\btd\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\conda_envs\\btd\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in d:\\conda_envs\\btd\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\conda_envs\\btd\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\conda_envs\\btd\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\conda_envs\\btd\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in d:\\conda_envs\\btd\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\conda_envs\\btd\\lib\\site-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in d:\\conda_envs\\btd\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in d:\\conda_envs\\btd\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in d:\\conda_envs\\btd\\lib\\site-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\conda_envs\\btd\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\conda_envs\\btd\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\conda_envs\\btd\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\conda_envs\\btd\\lib\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\conda_envs\\btd\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\conda_envs\\btd\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\conda_envs\\btd\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in d:\\conda_envs\\btd\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\conda_envs\\btd\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\conda_envs\\btd\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\conda_envs\\btd\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\conda_envs\\btd\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\conda_envs\\btd\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\conda_envs\\btd\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio einops pillow tqdm accelerate numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ec6c68-1c2f-4e78-9d44-f3b6ac5c7b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "epochs = 200  # Increase epochs as needed for better results\n",
    "image_size = 64  # Resize the images to 64x64 (you can increase to 128 or 256)\n",
    "learning_rate = 1e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a0c9c0-560f-41bf-af8f-f5183f2442fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your dataset\n",
    "tumor_dir = \"data/generation_data/Training/tumor\"  # Update with your path\n",
    "no_tumor_dir = \"data/generation_data/Training/notumor\"  # Update with your path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "986046f6-0b92-47a3-811e-32d05b97e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure grayscale images\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Custom Dataset class to load images\n",
    "class TumorDataset(Dataset):\n",
    "    def __init__(self, tumor_dir, no_tumor_dir, transform=None):\n",
    "        self.tumor_images = [os.path.join(tumor_dir, img) for img in os.listdir(tumor_dir)]\n",
    "        self.no_tumor_images = [os.path.join(no_tumor_dir, img) for img in os.listdir(no_tumor_dir)]\n",
    "        self.images = self.tumor_images + self.no_tumor_images\n",
    "        self.labels = [1] * len(self.tumor_images) + [0] * len(self.no_tumor_images)  # 1: Tumor, 0: No Tumor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        img = self.transform(img)  # Apply transformations\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "\n",
    "# Load dataset\n",
    "dataset = TumorDataset(tumor_dir, no_tumor_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634556c7-26a4-46ac-b413-6e4409ff18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPM model (U-Net architecture for diffusion)\n",
    "class UNetDDPM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetDDPM, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.middle = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc = self.encoder(x)\n",
    "        middle = self.middle(enc)\n",
    "        dec = self.decoder(middle)\n",
    "        return dec\n",
    "\n",
    "# Instantiate and move model to GPU/CPU\n",
    "model = UNetDDPM().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb28988f-28d6-483d-8a33-b80a2949436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (MSE for DDPM)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca70f15-24ae-48c8-aaa5-dd61cfe68acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(model, dataloader, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        loop = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        for batch_idx, (imgs, _) in enumerate(loop):\n",
    "            imgs = imgs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(imgs)\n",
    "            loss = loss_fn(output, imgs)  # Compare output with original image (DDPM)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Start training\n",
    "train(model, dataloader, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1b55d2-794e-4033-a933-9d7c7f9db2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model.state_dict(), \"unet_ddpm.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7659b5c0-bc58-43fa-ba9e-809442037756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate synthetic MRI images\n",
    "def generate_images(model, num_images=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_images, 1, image_size, image_size).to(device)  # Random noise\n",
    "        generated_images = model(z)  # Generate images\n",
    "        generated_images = generated_images.cpu().numpy()\n",
    "\n",
    "        for i, img in enumerate(generated_images):\n",
    "            plt.subplot(1, num_images, i + 1)\n",
    "            plt.imshow(img[0], cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Generate 5 synthetic images after training\n",
    "generate_images(model, num_images=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (btd)",
   "language": "python",
   "name": "btd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
